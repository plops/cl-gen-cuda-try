# Introduction

In this code I collect experiments with a code generator that emits C
code.  The code generator (https://github.com/plops/cl-cpp-generator)
is very simple. A single function consumes a lisp like language and
emits a string with C code.

Although very plain, this approach gives access to (some) of the power
of lisp macros. I can easily play around with different code
structures, try ideas fast, test the performance and throw non-working
attempts away without regret.


Currently I want to understand 2D Fast Fourier Transform. First on the
CPU and eventually perhaps on the GPU.

How should memory accesses be ordered?

Should twiddle factors be recomputed on the fly or tabulated? Or is a
mixed approach faster?

# Stages

First I made a 16 element FFT with radix 4 work. It computes row-wise DFTs of a 4x4 matrix, transposes and does it again.
This works on CPU.

Then I use this function to implement a 256 element FFT. Currently, this function isn't working.

# Caches





## Intel Core i5 CPU       M 520  @ 2.40GHz

- https://en.wikichip.org/wiki/intel/core_i5/i5-520m

|     |                         |                           |                                       |                    |
|-----+-------------------------+---------------------------+---------------------------------------+--------------------|
| L1I | 	64 KiB          | 	2x32 KiB          | 	4-way set associative	 | write-back         |
| L1D | 	64 KiB          | 	 2x32 KiB         | 	8-way set associative         | 	write-back |
| L2  | 	512 KiB	  | 2x256 KiB                 | 	8-way set associative         | 	write-back |
| L3  | 	3 MiB           | 		2x1.5 MiB | 	12-way set associative        | 	write-back |

## Intel x7-x8750

- https://en.wikichip.org/wiki/intel/atom_x7/z8750

|     |                 |                                                      |
|-----+-----------------+------------------------------------------------------|
| L1I | 	128 KiB | 	4x32 KiB 8-way set associative (per core)    |
| L1D | 	96 KiB  | 	4x24 KiB 6-way set associative (per core)    |
| L2  | 	2 MiB   | 	2x1 MiB 16-way set associative (per 2 cores) |
| L3  | 	0 KiB   | 	No L3                                        |




##  Intel Core i5-7400 CPU @ 3.00GHz

- https://en.wikichip.org/wiki/intel/core_i5/i5-7400

| L1I | 	128 KiB	 | 4x32 KiB          | 	8-way set associative  | 	           |
| L1D | 	128 KiB	 | 4x32 KiB          | 	8-way set associative  | 	write-back |
| L2  | 	1 MiB	   | 4x256 KiB         | 	4-way set associative  | 	write-back |
| L3  | 	6 MiB           | 	4x1.5 MiB | 	12-way set associative | 	write-back |


# Read performance counters

- rdpmc instruction https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/595214
  - PAPI overheads are typically in excess of 2000 cycles to read a single counter.  
  -  "perf stat" command (or similar) will sometimes use these fixed function counters and will disable them on exit

- use kernel parameters isolcpus=0 to free one of the cpus for benchmarking

# References

- cuda complex library https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp

- Vasily Volkov FFT on GPU (2008) https://people.eecs.berkeley.edu/~kubitron/courses/cs258-S08/projects/reports/project6_report.pdf
- https://github.com/vetter/shoc/blob/master/src/cuda/level1/fft/fftlib.cu
- http://www.bealto.com/gpu-fft_intro.html
- https://mc.stanford.edu/cgi-bin/images/7/75/SC08_FFT_on_GPUs.pdf
- eliminate power-of-two memory stride for better caching https://www.davidhbailey.com/dhbpapers/fftzp.pdf
- how to split dft into 2d https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm

- gcc vectorization https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html
- profile mem access (2007) https://lwn.net/Articles/257209/
  - opannotate .. lists the source or assembler code of the program
    and shows the instructions where the event was recognized
- how to use perf https://www.youtube.com/watch?v=M6ldFtwWup0
- https://github.com/RRZE-HPC/likwid/wiki
- http://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html


- good explanation how to measure and interprete timing http://sites.utexas.edu/jdm4372/2018/07/23/comments-on-timing-short-code-sections-on-intel-processors/
- https://github.com/jdmccalpin/low-overhead-timers
- https://github.com/jdmccalpin/periodic-performance-counters
- offloading https://www.youtube.com/watch?v=kIA_UtdVabQ
  - #pragma omp target teams distribute parallel for collapse(2) map(to:zr,zi,xscale,yscale) map(from:results[0:npixels])
- https://software.intel.com/en-us/forums/intel-isa-extensions/topic/289038
  - working prefetch increases the cache misses (but speeds up execution)
  - resource stall ratio: RS_FULL ratio and ROB_FULL ratio
  - Frequent resource stalls indicate the presence of tuning opportunities like frequent cache misses, long execution paths, and memory order buffer (MOB) stalls
- https://www.blackhat.com/docs/us-15/materials/us-15-Herath-These-Are-Not-Your-Grand-Daddys-CPU-Performance-Counters-CPU-Hardware-Performance-Counters-For-Security.pdf
  - Performance Application Programming Interface http://icl.cs.utk.edu/papi/Perfmon2 http://perfmon2.sourceforge.net/
  - Andi Kleen's pmu-tools https://github.com/andikleen/pmu-tools
    - toplev .. Estimate on which part of the CPU pipeline a workload bottlenecks using the TopDown model
    -  sudo ./toplev.py --all --core C0 taskset -c 0,1 ~/stage/cl-gen-cuda-try/source/cpu_try_gcc
    - http://www.cs.technion.ac.il/~erangi/TMA_using_Linux_perf__Ahmad_Yasin.pdf
  - LikwidPerfCtr https://github.com/rrze-likwid/likwid
